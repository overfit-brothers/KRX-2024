{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from litellm import completion, batch_completion\n",
    "import os\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_kor = \"\"\"\n",
    "당신의 일은 문제-정답 쌍으로 되어 있는 데이터셋이 주어진 목적에 부합한 지 판단하는 것입니다.\n",
    "주어진 데이터를 읽고 다음 기준에 얼마나 부합하는 지 1~5점 사이의 점수를 매기세요.\n",
    "\n",
    "*** 기준 : \n",
    "1. 금융학, 투자 전략, 법률, 금융 관련 수학 계산 등 금융 관련 주제에 대한 데이터인지\n",
    "2. 전문적인 분석과 논리적인 사고를 요구하거나 포함되어있는 데이터인지\n",
    "3. (문제의 경우) 문제에서 요구하는 내용이 구체적이고 명확하며, 문제 해결에 필요한 정보가 충분한지\n",
    "4. (정답의 경우) 정답이 정확하고 완전하며, Chain of Thoughts 방식으로 정답을 도출되었는지\n",
    "\n",
    "*** 점수 기준\n",
    "\n",
    "5점: \n",
    "- 금융 전문 지식이 필요하며 문제와 정답이 매우 구체적이고 완벽함\n",
    "- 논리적 추론 과정이 명확하게 단계별로 제시됨\n",
    "- 제시한 기준과 매우 적합함\n",
    "\n",
    "4점:\n",
    "- 금융 관련 내용을 포함하며 분석적 사고가 필요함\n",
    "- 문제와 정답이 구체적이고 추론 과정이 있음\n",
    "- 제시한 기준과 대부분 적합함\n",
    "\n",
    "3점:\n",
    "- 금융과 간접적으로 연관되어 있음\n",
    "- 기본적인 분석만 필요하고 추론 과정이 불완전하거나 부족함\n",
    "\n",
    "2점:\n",
    "- 금융 관련성이 매우 낮고 단순 암기 수준임\n",
    "- 문제나 정답이 모호하고 추론 과정 없음\n",
    "\n",
    "1점:\n",
    "- 금융과 전혀 무관한 주제임\n",
    "- 문제와 정답이 부적절하거나 불명확함\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to evaluate whether question-answer pairs in a dataset meet the given purpose.\n",
    "Read the given data and score it on a scale of 1-5 based on how well it meets the following criteria:\n",
    "\n",
    "*** Evaluation Criteria:\n",
    "\n",
    "Content Categories:\n",
    "\n",
    "1. Financial Expertise\n",
    "- Relevance to financial topics (studies, investment, law, calculations)\n",
    "- Professional analysis and expertise requirements\n",
    "- Market efficiency and investor value contribution\n",
    "2. Logical Structure\n",
    "- Clear analytical thinking process\n",
    "- Step-by-step reasoning\n",
    "- Evidence-based conclusions\n",
    "3. Practical Value\n",
    "- Market applicability\n",
    "- Investor usefulness\n",
    "- Real-world implementation\n",
    "4. Format & Compliance\n",
    "- Natural Korean language usage\n",
    "- Legal/ethical compliance\n",
    "- Professional terminology accuracy\n",
    "\n",
    "*** Scoring Guidelines:\n",
    "\n",
    "5 points (Outstanding):\n",
    "- Demonstrates advanced financial expertise\n",
    "- Perfect step-by-step logical reasoning\n",
    "- Highly practical and implementable solutions\n",
    "- Complete alignment with all criteria\n",
    "\n",
    "4 points (Proficient):\n",
    "- Shows solid financial knowledge\n",
    "- Clear logical progression\n",
    "- Practical application potential\n",
    "- Meets most evaluation criteria\n",
    "\n",
    "3 points (Adequate):\n",
    "- Basic financial relevance\n",
    "- Partial analytical process\n",
    "- Limited practical value\n",
    "- Meets basic requirements\n",
    "\n",
    "2 points (Limited):\n",
    "- Minimal financial connection\n",
    "- Lacks analytical depth\n",
    "- No practical application\n",
    "- Major criteria gaps\n",
    "\n",
    "1 point (Insufficient):\n",
    "- No financial relevance\n",
    "- Unclear or inappropriate content\n",
    "- No analytical value\n",
    "- Fails to meet basic criteria\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "from litellm import completion, batch_completion\n",
    "import os\n",
    "import litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List,Literal\n",
    "import json\n",
    "class AnswerModel(BaseModel):\n",
    "    score : Literal[\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-XXX\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 35431/35431 [00:00<00:00, 78141.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = 'overfit-brothers/filtered-merged-50k-finqa'\n",
    "\n",
    "raw = load_dataset(data_path)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'reference'],\n",
       "    num_rows: 35431\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrys = []\n",
    "cnt = 0\n",
    "for t in raw:\n",
    "    question = t['question']\n",
    "    answer = t['answer']\n",
    "    system_prompt = SYSTEM_PROMPT\n",
    "    prompt = f'Question : {question}\\nAnswer : {answer}'\n",
    "    messages = [\n",
    "    {\"content\": system_prompt, \"role\": \"system\"},\n",
    "    {\"content\": prompt, \"role\": \"user\"}]\n",
    "    qrys.append(messages)\n",
    "    # cnt+=1\n",
    "    # if cnt ==100:\n",
    "    #     break\n",
    "# 1. raw text 데이터를 활용한 질문 생성\n",
    "responses = batch_completion(\n",
    "    model=\"gpt-4o\",\n",
    "    messages = qrys,\n",
    "    response_format=AnswerModel,\n",
    ")\n",
    "response_resps = [json.loads(i.choices[0].message.content) for i in responses]\n",
    "total_prompt_tokens_for_q = sum([r.usage.prompt_tokens for r in responses])\n",
    "total_completion_tokens_for_q = sum([r.usage.completion_tokens for r in responses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total prompt tokens: 34181072\n",
      "prompt token costs: $ 85.45268\n",
      "total completion tokens: 177155\n",
      "completion token costs: $ 1.77155\n"
     ]
    }
   ],
   "source": [
    "print('total prompt tokens:', total_prompt_tokens_for_q )\n",
    "print('prompt token costs: $', round((total_prompt_tokens_for_q ) / 1000000 * 2.5, 6))\n",
    "print('total completion tokens:', total_completion_tokens_for_q )\n",
    "print('completion token costs: $', round((total_completion_tokens_for_q ) / 1000000 * 10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sample = raw.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1: 47 counts\n",
      "Score 2: 291 counts\n",
      "Score 3: 1189 counts\n",
      "Score 4: 7454 counts\n",
      "Score 5: 26450 counts\n"
     ]
    }
   ],
   "source": [
    "sample_list = []\n",
    "score_counts = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}\n",
    "for score, datum in zip(response_resps,raw):\n",
    "    score_value = int(score['score'])\n",
    "    if 1 <= score_value <= 5:\n",
    "        score_counts[score_value] += 1\n",
    "    datum['score'] = score_value\n",
    "    if score_value == 5:\n",
    "        sample_list.append(datum)\n",
    "\n",
    "for score in range(1, 6):\n",
    "    print(f\"Score {score}: {score_counts[score]} counts\")\n",
    "    \n",
    "dataset = Dataset.from_list(sample_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'reference', 'score'],\n",
       "    num_rows: 26450\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 27/27 [00:00<00:00, 63.45ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/overfit-brothers/filtered-merged-50k-finqa_quality_filtered/commit/75c87dbed650fe83bde7718ab18b8d7dfc88a3aa', commit_message='Upload dataset', commit_description='', oid='75c87dbed650fe83bde7718ab18b8d7dfc88a3aa', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/overfit-brothers/filtered-merged-50k-finqa_quality_filtered', endpoint='https://huggingface.co', repo_type='dataset', repo_id='overfit-brothers/filtered-merged-50k-finqa_quality_filtered'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"overfit-brothers/filtered-merged-50k-finqa_quality_filtered\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '주 6: 재고는 미국 대륙에 위치한 대부분의 재고에 대해 후입선출(lifo) 방식을 사용하며, 그 외의 재고는 후입선출(first-in, first-out) 방식으로 평가된다. 피포 비용은 현재의 대체 비용과 비슷하다. lifo를 이용하여 측정한 재고는 반드시 원가 또는 시장 중에서 낮은 가격으로 평가되어야 한다. 피포를 사용하여 측정한 재고자산은 원가 또는 순실현가능가치 중 낮은 값으로 평가되어야 한다. 12월 31일 재고는 다음과 같이 구성되었다.\\n\\n- | 2018 | 2017\\n완제품 | $988.1 | $1211.4\\n가공 중 | 2628.2 | 2697.7\\n원자재 및 용품 | 506.5 | 488.8\\n총 (대략적인 교체 비용) | 4122.8 | 4397.9\\n라이프 비용으로 증가(감소) | -11.0 (11.0) | 60.4\\n재고 | $4111.8 | $4458.3\\n\\n2018년 12월 31일과 2017년 12월 31일에는 각각 15억 7천만 달러와 15억 6천만 달러의 총재고를 기록하였다. 주 7: 잠재적으로 신용위험을 받는 금융상품은 주로 매출채권과 이자부담 투자로 구성된다. 생명과학 제품의 도매 유통업자는 우리의 매출채권의 상당 부분을 차지하며, 일반적으로 담보는 필요하지 않습니다. 우리는 지속적인 신용평가 절차와 보험을 통해 이러한 집중과 관련된 위험을 완화하고자 한다. 우리의 현금의 상당 부분은 몇몇 주요 금융기관에 의해 보유되고 있다. 우리는 이들 기관과 함께 우리의 익스포저를 모니터링하며, 이들 기관 중 어느 기관도 의무를 이행하지 않을 것으로 기대하지 않는다. 주요 금융기관은 우리가 기업 부채 증권에 투자하는 가장 큰 부분을 대표한다. 문서화된 기업 리스크 관리 정책에 따라, 우리는 한 금융 기관 또는 기업 발행자의 신용 노출량을 모니터링한다. 우리는 위험관리 수단에 대한 거래상대방의 불이행 시 신용 관련 손실에 노출되어 있지만 높은 신용등급을 감안할 때 어떤 거래상대방도 의무를 이행하지 않을 것으로 예상하지는 않는다. 우리는 구매일로부터 3개월 이하의 만기를 가진 모든 유동성이 높은 투자를 현금 등가물로 간주한다. 이러한 투자의 비용은 공정가치에 근접한다. 우리의 지분 투자는 지분 투자 유형에 따라 세 가지 다른 방법을 사용하여 설명된다: 우리가 상당한 영향력을 가지고 있지만 지배 지분이 없는 회사에 대한 2022년 투자는 지분 방법을 사용하여 설명되며, 우리의 수익 또는 손실 비율은 기타 순, (이익) 비용으로 보고된다. 2022년에는 쉽게 결정할 수 있는 공정가치가 없는 주식투자의 경우 동일 발행자의 동일 또는 유사한 투자에 대해 질서 있는 거래에서 관찰 가능한 가격변동에 따른 손상을 제외하고 이러한 투자를 원가로 측정한다. 기록된 가치의 변동은 기타순손익, (수익)비용에 계상한다. 2022년 우리의 공공 지분 투자는 공정한 가치로 측정되고 이월됩니다. 공정가치의 변동은 기타순손익으로 인식한다. 우리는 정기적으로 손상 징후에 대해 공공 지분 투자 이외의 지분 투자를 검토한다. 우리의 파생상품 활동은 문서화된 기업 위험관리 정책의 지침 내에서 시작되며, 위험회피 대상 자산, 부채 및 거래의 손실과 이익을 상쇄하기 위한 것이다. 경영진은 분기별로 우리의 파생상품의 상관관계와 효과를 검토한다..\\n\\n2018년 원자재 및 물량 총계는? 506.5\\n\\n2017년에는 뭐였지? 488.8\\n\\n그렇다면, 1년동안 어떤 변화가 있었는가? 17.7\\n\\n2017년 원자재 및 물량 총계는? 488.8\\n\\n그리고 그 변화는 이 2017년 총계와 관련하여 얼마를 나타내는가, 백분율로?',\n",
       " 'answer': '문제를 이해하기 위해 주어진 정보를 정리합니다.\\n\\n먼저, 2017년의 원자재 및 용품의 총계는 \\\\$488.8입니다.  \\n\\n2018년의 원자재 및 용품의 총계는 \\\\$506.5입니다.\\n\\n이로부터 1년 동안의 변화는 \\\\$506.5 - \\\\$488.8 = \\\\$17.7입니다.\\n\\n이 변화가 2017년의 총계에 대해 차지하는 비율을 백분율로 계산하려면 다음과 같이 진행합니다.\\n\\n변화율을 구하기 위해, 변화량을 2017년의 값으로 나눕니다:  \\n\\\\[ \\\\text{변화율} = \\\\left(\\\\frac{\\\\$17.7}{\\\\$488.8}\\\\right) \\\\times 100 \\\\]\\n\\n계산을 수행하면:\\n\\\\[ \\\\text{변화율} = 3.623\\\\% \\\\] (소수점 둘째 자리까지 반올림)\\n\\n따라서 정답은 3.623%입니다.',\n",
       " 'reference': 'overfit-brothers/ConvFinQA-ko-cot',\n",
       " 'score': 5}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('score_sample.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(sample_list,f,ensure_ascii=False,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
